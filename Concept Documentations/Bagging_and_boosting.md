# Bragging and boosting

## Documentation

Bagging

- bagging by random forest classifier  
- random subsets ```trained in parallel```
- ```reduce variance```

Boosting

- ```reduce variance and bias```
- boosting by adaboost and xgboost
ADA BOOST
- ```fixes misclassification sequentially``` and decisionstumps
XGBOOST
- Uses regularization and tree pruning
- each tree fits to error of previous prediction
- XG boost is usually more sensitive to hyperparameters.

## Output

Accuracy of random forest 0.9649122807017544
Accuracy of xgb 0.956140350877193
Accuracy of adaboosting 0.9736842105263158

Accuracy of AdaBoosting was highest, then random forest bagging and last XGboosting.
